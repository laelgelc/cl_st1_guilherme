{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810d9f10-ec19-4b09-8f90-e983e460b319",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<center>\n",
    "<img src=\"https://laelgelcpublic.s3.sa-east-1.amazonaws.com/lael_50_years_narrow_white.png.no_years.400px_96dpi.png\" width=\"300\" alt=\"LAEL 50 years logo\">\n",
    "<h3>APPLIED LINGUISTICS GRADUATE PROGRAMME (LAEL)</h3>\n",
    "</center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c2c96-2fc3-4a1a-995b-c388036a2a15",
   "metadata": {},
   "source": [
    "# Corpus Linguistics - Study 1 - Guilherme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3ad8a7-2d34-476b-be69-5d896b27d3ed",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1209cf-0515-45b3-8856-275ac692943e",
   "metadata": {},
   "source": [
    "Make sure the prerequisites in [CL_LMDA_prerequisites](https://github.com/laelgelc/laelgelc/blob/main/CL_LMDA_prerequisites.ipynb) are satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb36253-6dbb-4090-ba04-b450fcc59dea",
   "metadata": {},
   "source": [
    "### Additional prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e417cfbf-3218-4cb5-82ef-1bb087ae940f",
   "metadata": {},
   "source": [
    "#### WebVTT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415db25-e345-4d57-abbf-433eddd25da5",
   "metadata": {},
   "source": [
    "`webvtt-py` is a Python package for reading/writing WebVTT caption files.\n",
    "\n",
    "Please refer to:\n",
    "- [webvtt-py](https://pypi.org/project/webvtt-py/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6ae92b-4e3c-4809-87aa-b2b04072af49",
   "metadata": {},
   "source": [
    "##### Installing `webvtt-py` on Anacoda Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc6b9f0-b719-44e8-906f-18736146c840",
   "metadata": {},
   "source": [
    "As `webvtt-py` is not available in any of the conda channels, the following procedure should be followed on `Anaconda Prompt` to install it in the required environment, in this case `Env20240401`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc5cf3-96d1-4938-98ce-9e3f9208d5a0",
   "metadata": {},
   "source": [
    "Note:\n",
    "- You have to download and open this Jupyter Notebook on JupyterLab (provided as part of Anaconda Distribution) to visualise the procedure\n",
    "- Replace `Env20240401` by your actual environment name"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f176d912-e765-4670-b081-afad876b38fb",
   "metadata": {},
   "source": [
    "(base) C:\\Users\\eyamr>conda env list\n",
    "# conda environments:\n",
    "#\n",
    "base                  *  C:\\Users\\eyamr\\anaconda3\n",
    "Env20240401              C:\\Users\\eyamr\\anaconda3\\envs\\Env20240401\n",
    "\n",
    "\n",
    "(base) C:\\Users\\eyamr>conda activate Env20240401\n",
    "\n",
    "(Env20240401) C:\\Users\\eyamr>pip3 install webvtt-py\n",
    "<omitted>\n",
    "\n",
    "(Env20240401) C:\\Users\\eyamr>pip3 freeze\n",
    "<omitted>\n",
    "webvtt-py==0.5.0\n",
    "<omitted>\n",
    "\n",
    "(Env20240401) C:\\Users\\eyamr>conda deactivate\n",
    "\n",
    "(base) C:\\Users\\eyamr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7190c94a-63f0-404e-ba80-9e85d24aeb6f",
   "metadata": {},
   "source": [
    "## WebVTT proof of concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc18bc6-b828-4338-afb7-b325da71b7c7",
   "metadata": {},
   "source": [
    "Please refer to:\n",
    "- [CL_webvtt-py_Extraction](https://github.com/laelgelc/laelgelc/blob/main/CL_webvtt-py_Extraction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28897518-a858-4dc5-b510-49c9131da966",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f58941-1578-42c0-8933-d8ebef369245",
   "metadata": {},
   "source": [
    "Please download the following dataset (Right-click on the link and choose `Save link as` to download the corresponding file):\n",
    "- [cl_st1_guilherme-dataset.zip](https://laelgelcguilherme.s3.sa-east-1.amazonaws.com/cl_st1_guilherme-dataset.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9457dffb-6b09-4690-b407-6bb1646870f9",
   "metadata": {},
   "source": [
    "Extract the .zip file in the directory where this Jupyter Notebook is being executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c2af7-9fc1-4f51-a4f5-2ed915b93039",
   "metadata": {},
   "source": [
    "## Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6689499-d19d-4b0c-befd-f88926b7105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webvtt\n",
    "import pandas as pd\n",
    "import demoji\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a79a002-c182-4021-9b77-99ad470582af",
   "metadata": {},
   "source": [
    "## Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d1ee95-44d8-4613-8e6e-efb4ca802da7",
   "metadata": {},
   "source": [
    "### Defining the input and output directory names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e75417e-31bb-482c-b430-503a6d4605af",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = 'cl_st1_guilherme-dataset'\n",
    "output_directory = input_directory + '-output'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49faf5ab-cef2-4edd-b513-e94cbb6f56bc",
   "metadata": {},
   "source": [
    "### Defining a function to extract caption texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "922dade8-e111-4a62-9b93-4d4c511cbd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_caption_text(webvtt_file, caption_file):\n",
    "    vtt = webvtt.read(webvtt_file)\n",
    "    \n",
    "    # Writing the text of the caption to the output file\n",
    "    with open(caption_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('text' + '\\n') # Includes the header that will be used in the dataframe\n",
    "        for caption in vtt:\n",
    "            f.write(caption.text + '\\n')\n",
    "    \n",
    "    # Deduplicating the text of the caption using a dataframe\n",
    "    df = pd.read_table(caption_file)\n",
    "    df['text'] = df['text'].map(str)\n",
    "    df.drop_duplicates(subset='text', keep='first', inplace=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Creating a single string containing all 'text' values separated by spaces\n",
    "    text_line = ' '.join(df['text'])\n",
    "\n",
    "    # Rewriting the output file with the single string\n",
    "    with open(caption_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(text_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c301b7c-f3ce-4339-8940-7b2b26da016a",
   "metadata": {},
   "source": [
    "### Defining a function to recursively process the `input_directory` and store the results in `output_directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e1f78e-54a2-470d-847c-67dd2e577213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(input_directory, output_directory):\n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.vtt'):\n",
    "                # Constructing the corresponding caption filename\n",
    "                base_name = os.path.splitext(filename)[0]\n",
    "                caption_filename = base_name + '.txt'\n",
    "\n",
    "                # Creating the output subdirectory structure\n",
    "                relative_path = os.path.relpath(root, input_directory)\n",
    "                output_subdir = os.path.join(output_directory, relative_path)\n",
    "                os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "                # Full paths for input and output files\n",
    "                input_file_path = os.path.join(root, filename)\n",
    "                output_file_path = os.path.join(output_subdir, caption_filename)\n",
    "\n",
    "                # Calling 'extract_caption_text' function\n",
    "                extract_caption_text(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9a05d4-52d0-4f88-b678-65a9db6a0608",
   "metadata": {},
   "source": [
    "### Processing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88894009-831b-48c8-9c1b-604737045e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_directory(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697bf984-0bbf-49c0-9524-e90a0b2c60c3",
   "metadata": {},
   "source": [
    "### Importing the texts into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c6fde50-9149-4d29-b249-e891a7917bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_contents(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f'Error reading file {file_path}: {e}')\n",
    "        return None\n",
    "\n",
    "def process_output_directory(output_directory):\n",
    "    # Initialize an empty list to store data\n",
    "    data = []\n",
    "\n",
    "    # Recursively iterate through the output_directory\n",
    "    for root, _, files in os.walk(output_directory):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            file_contents = read_file_contents(file_path)\n",
    "            if file_contents is not None:\n",
    "                data.append({'text': file_contents, 'filepath': file_path})\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Importing the texts into the dataframe 'df_tweets_filtered'. Even though this study does not relate to 'tweets', this dataframe name is adopted in order to enable code reuse in subsequent processing stages\n",
    "df_tweets_filtered = process_output_directory(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63149dc4-2772-4136-ab2d-a43c9d6da299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>povo abençoado do Brasil há mais de 2 anos ins...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\hashtagFALAMAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>estou emem Alagoas e primeiro eu quero agradec...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\hashtagFALAMAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>povo abençoado do Brasil Preste bastante atenç...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\hashtagFALAMAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>povo abençoado do Brasil quando a gente pensa ...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\hashtagFALAMAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Olá povo abençoado do Brasil rapaz onde nós ch...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\hashtagFALAMAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>o deserto ninguém gosta dele deserto simboliza...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>e guarde a sua mente o que domina sua mente a ...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>em Gênesis Capítulo 6 Versículo 8 Oi Noé porém...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>a uma pessoa determinada é uma pessoa que sabe...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>é a fé que podemos ter e a fé do apóstolo Paul...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1826 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     povo abençoado do Brasil há mais de 2 anos ins...   \n",
       "1     estou emem Alagoas e primeiro eu quero agradec...   \n",
       "2     povo abençoado do Brasil Preste bastante atenç...   \n",
       "3     povo abençoado do Brasil quando a gente pensa ...   \n",
       "4     Olá povo abençoado do Brasil rapaz onde nós ch...   \n",
       "...                                                 ...   \n",
       "1821  o deserto ninguém gosta dele deserto simboliza...   \n",
       "1822  e guarde a sua mente o que domina sua mente a ...   \n",
       "1823  em Gênesis Capítulo 6 Versículo 8 Oi Noé porém...   \n",
       "1824  a uma pessoa determinada é uma pessoa que sabe...   \n",
       "1825  é a fé que podemos ter e a fé do apóstolo Paul...   \n",
       "\n",
       "                                               filepath  \n",
       "0     cl_st1_guilherme-dataset-output\\hashtagFALAMAL...  \n",
       "1     cl_st1_guilherme-dataset-output\\hashtagFALAMAL...  \n",
       "2     cl_st1_guilherme-dataset-output\\hashtagFALAMAL...  \n",
       "3     cl_st1_guilherme-dataset-output\\hashtagFALAMAL...  \n",
       "4     cl_st1_guilherme-dataset-output\\hashtagFALAMAL...  \n",
       "...                                                 ...  \n",
       "1821  cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...  \n",
       "1822  cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...  \n",
       "1823  cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...  \n",
       "1824  cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...  \n",
       "1825  cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...  \n",
       "\n",
       "[1826 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b9f3a-5604-4efa-82a5-780b9b1f7837",
   "metadata": {},
   "source": [
    "### Exporting the filtered data into a file for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c99e0e64-6977-47d0-a15a-9ea1abce8951",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered.to_csv('tweets_emojified.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d8c1a-2600-4b18-a1c7-0141fdcd40ea",
   "metadata": {},
   "source": [
    "## Replacing emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3478255c-a7a8-48f0-9608-999dc92464a4",
   "metadata": {},
   "source": [
    "### Demojifying the column `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2341baec-2fa0-4ee8-82cf-1522257ab02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to demojify a string\n",
    "def demojify_line(input_line):\n",
    "    demojified_line = demoji.replace_with_desc(input_line, sep='<em>')\n",
    "    return demojified_line\n",
    "\n",
    "df_tweets_filtered['text'] = df_tweets_filtered['text'].apply(demojify_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08405272-4072-4cbe-92c0-c221d3848181",
   "metadata": {},
   "source": [
    "#### Exporting the filtered data into a file for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19a2453a-9a6a-4811-ad67-4a7ad495d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered.to_csv('tweets_demojified1.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820ee493-3e57-4d46-817d-b7a4af0b0722",
   "metadata": {},
   "source": [
    "### Separating the demojified strings with spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d55d111-b676-4569-89a0-a1c4ec2e57be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to separate the demojified strings with spaces\n",
    "def preprocess_line(input_line):\n",
    "    # Add a space before the first delimiter '<em>', if it is not already preceded by one\n",
    "    preprocessed_line = re.sub(r'(?<! )<em>', ' <em>', input_line)\n",
    "    # Add a space after the first delimiter '<em>', if it is not already followed by one\n",
    "    preprocessed_line = re.sub(r'<em>(?! )', '<em> ', preprocessed_line)\n",
    "    return preprocessed_line\n",
    "\n",
    "# Separating the demojified strings with spaces\n",
    "df_tweets_filtered['text'] = df_tweets_filtered['text'].apply(preprocess_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb282a-30a2-4401-80ac-305aa5f9d7c5",
   "metadata": {},
   "source": [
    "#### Exporting the filtered data into a file for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa3fce08-90f6-4e6a-97f7-ae69517a3689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered.to_csv('tweets_demojified2.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b2004-f565-44f9-9d43-31bf393170d8",
   "metadata": {},
   "source": [
    "### Formatting the demojified strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2357ac57-bf81-4377-9a21-90fb79d1fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to format the demojified string\n",
    "def format_demojified_string(input_line):\n",
    "    # Defining a function to format the demojified string using RegEx\n",
    "    def process_demojified_string(s):\n",
    "            # Lowercase the string\n",
    "            s = s.lower()\n",
    "            # Replace spaces and colons followed by a space with underscores\n",
    "            s = re.sub(r'(: )| ', '_', s)\n",
    "            # Add the appropriate prefixes and suffixes\n",
    "            s = f'EMOJI{s}e'\n",
    "            return s\n",
    "\n",
    "    # Use RegEx to find and process each demojified string\n",
    "    processed_line = re.sub(r'<em>(.*?)<em>', lambda match: process_demojified_string(match.group(1)), input_line)\n",
    "    return processed_line\n",
    "\n",
    "# Formatting the demojified strings\n",
    "df_tweets_filtered['text'] = df_tweets_filtered['text'].apply(format_demojified_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e743b0db-02a0-4e0b-b0b5-d6659960bc30",
   "metadata": {},
   "source": [
    "### Replacing the `pipe` character by the `-` character in the `text` column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc943030-7448-4d4d-98b8-b03c40e113e2",
   "metadata": {},
   "source": [
    "Further on, a few columns of the dataframe are going to be exported into the file `tweets.txt` whose columns need to be delimited by the `pipe` character. Therefore, it is recommended that any occurrences of the `pipe` character in the `text` column are replaced by another character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab1c3584-ae80-41fc-b03a-4286a564a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to replace the 'pipe' character by the '-' character\n",
    "def replace_pipe_with_hyphen(input_string):\n",
    "    modified_string = re.sub(r'\\|', '-', input_string)\n",
    "    return modified_string\n",
    "\n",
    "# Replacing the 'pipe' character by the '-' character\n",
    "df_tweets_filtered['text'] = df_tweets_filtered['text'].apply(replace_pipe_with_hyphen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3d28f3-7d63-4c02-ba55-0dbc957a4291",
   "metadata": {},
   "source": [
    "#### Exporting the filtered data into a file for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aab47cc6-5888-418a-8e4f-97daaf2ac3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered.to_csv('tweets_demojified3.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d4ca4-2999-47d3-ae97-b4f3a7599d84",
   "metadata": {},
   "source": [
    "## Tokenising"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f950bc7-9ffa-4d96-9ce0-439643f90a55",
   "metadata": {},
   "source": [
    "Please refer to [What is tokenization in NLP?](https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c1d393e-e85d-48f8-93f2-d7dfec6a2062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to tokenise a string\n",
    "def tokenise_string(input_line):\n",
    "    # Replace URLs with placeholders\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\\b'\n",
    "    placeholder = '<URL>'  # Choose a unique placeholder\n",
    "    urls = re.findall(url_pattern, input_line)\n",
    "    tokenised_line = re.sub(url_pattern, placeholder, input_line)  # Replace URLs with placeholders\n",
    "    \n",
    "    # Replace curly quotes with straight ones\n",
    "    tokenised_line = tokenised_line.replace('“', '\"').replace('”', '\"').replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "    # Separate common punctuation marks with spaces\n",
    "    tokenised_line = re.sub(r'([.\\!?,\"\\'/()])', r' \\1 ', tokenised_line)\n",
    "    # Add a space before '#'\n",
    "    tokenised_line = re.sub(r'(?<!\\s)#', r' #', tokenised_line)  # Add a space before '#' if it is not already preceded by one\n",
    "    # Reduce extra spaces by a single space\n",
    "    tokenised_line = re.sub(r'\\s+', ' ', tokenised_line)\n",
    "    \n",
    "    # Replace the placeholders with the respective URLs\n",
    "    for url in urls:\n",
    "        tokenised_line = tokenised_line.replace(placeholder, url, 1)\n",
    "    \n",
    "    return tokenised_line\n",
    "\n",
    "# Tokenising the strings\n",
    "df_tweets_filtered['text'] = df_tweets_filtered['text'].apply(tokenise_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad8cc4-bd36-4d7a-905e-55173081cd85",
   "metadata": {},
   "source": [
    "## Creating the files `file_index.txt` and `tweets.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a908ea-85cd-4ded-b752-2ce8ccee6f62",
   "metadata": {},
   "source": [
    "### Creating column `text_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e69b8a02-8d66-4227-838a-020f8e458225",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered['text_id'] = 't' + df_tweets_filtered.index.astype(str).str.zfill(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ecbc7-802a-4546-b770-bd94372e58f7",
   "metadata": {},
   "source": [
    "### Creating column `conversation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ad070c7-3caf-4eb8-8dfe-eba469fe0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered['conversation'] = 'v:' + df_tweets_filtered['filepath']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85be5b3-f3ec-4061-a9c7-6d29591a8efd",
   "metadata": {},
   "source": [
    "### Creating column `date`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d877e-c1c7-45c2-b624-0200a8c4a911",
   "metadata": {},
   "source": [
    "The date for all texts are defined as the date Guilherme sent the dataset, 16th April, 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85fd8522-d306-4218-9d76-63993ff1ed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered['date'] = 'd:' + '2024-04-16'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067eec7-aedb-4bba-8241-5d5d000f6226",
   "metadata": {},
   "source": [
    "### Creating column `text_url`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51df231d-a251-4e92-ac97-1aa5d7e00a42",
   "metadata": {},
   "source": [
    "No URL was considered for all texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a5dec9e-0791-4b40-b59e-b46b62d400ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered['text_url'] = 'url:' + 'no_url'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c35b3-69bc-4039-aa13-6b496f81c729",
   "metadata": {},
   "source": [
    "### Creating column `user`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8956e4-6123-4c6a-ad0d-a99dfbe2b40d",
   "metadata": {},
   "source": [
    "`silas_malafaia` was considered for all texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5700d8af-3be8-4a60-bd3d-1ddf3df356ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered['user'] = 'u:' + 'silas_malafaia'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1d2a83-b1a0-4db7-b9d3-e6d91202402c",
   "metadata": {},
   "source": [
    "### Creating column `content`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd9313bd-6cbe-487c-a152-de678559260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered['content'] = 'c:' + df_tweets_filtered['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11872fa-2b44-4302-ab43-f98f64692eaf",
   "metadata": {},
   "source": [
    "### Reordering the created columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1538070e-1838-454f-a761-549e2bd5bc36",
   "metadata": {},
   "source": [
    "Please refer to:\n",
    "- [Python - List Comprehension 1](https://www.w3schools.com/python/python_lists_comprehension.asp)\n",
    "- [Python - List Comprehension 2](https://treyhunner.com/2015/12/python-list-comprehensions-now-in-color/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afc325f4-43c7-4ba8-b10f-130974d8c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the columns (we use list comprehension to create a list of all columns except 'text_id', 'variable', 'date' and 'text_url')\n",
    "df_tweets_filtered = df_tweets_filtered[['text_id', 'conversation', 'date', 'text_url', 'user', 'content'] + [col for col in df_tweets_filtered.columns if col not in ['text_id', 'conversation', 'date', 'text_url', 'user', 'content']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1217907f-187c-4d64-a97a-e7a9c607f6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>conversation</th>\n",
       "      <th>date</th>\n",
       "      <th>text_url</th>\n",
       "      <th>user</th>\n",
       "      <th>content</th>\n",
       "      <th>text</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t000000</td>\n",
       "      <td>v:cl_st1_guilherme-dataset-output\\hashtagFALAM...</td>\n",
       "      <td>d:2024-04-16</td>\n",
       "      <td>url:no_url</td>\n",
       "      <td>u:silas_malafaia</td>\n",
       "      <td>c:povo abençoado do Brasil há mais de 2 anos i...</td>\n",
       "      <td>povo abençoado do Brasil há mais de 2 anos ins...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\hashtagFALAMAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t000001</td>\n",
       "      <td>v:cl_st1_guilherme-dataset-output\\hashtagFALAM...</td>\n",
       "      <td>d:2024-04-16</td>\n",
       "      <td>url:no_url</td>\n",
       "      <td>u:silas_malafaia</td>\n",
       "      <td>c:estou emem Alagoas e primeiro eu quero agrad...</td>\n",
       "      <td>estou emem Alagoas e primeiro eu quero agradec...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\hashtagFALAMAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t000002</td>\n",
       "      <td>v:cl_st1_guilherme-dataset-output\\hashtagFALAM...</td>\n",
       "      <td>d:2024-04-16</td>\n",
       "      <td>url:no_url</td>\n",
       "      <td>u:silas_malafaia</td>\n",
       "      <td>c:povo abençoado do Brasil Preste bastante ate...</td>\n",
       "      <td>povo abençoado do Brasil Preste bastante atenç...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\hashtagFALAMAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t000003</td>\n",
       "      <td>v:cl_st1_guilherme-dataset-output\\hashtagFALAM...</td>\n",
       "      <td>d:2024-04-16</td>\n",
       "      <td>url:no_url</td>\n",
       "      <td>u:silas_malafaia</td>\n",
       "      <td>c:povo abençoado do Brasil quando a gente pens...</td>\n",
       "      <td>povo abençoado do Brasil quando a gente pensa ...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\hashtagFALAMAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t000004</td>\n",
       "      <td>v:cl_st1_guilherme-dataset-output\\hashtagFALAM...</td>\n",
       "      <td>d:2024-04-16</td>\n",
       "      <td>url:no_url</td>\n",
       "      <td>u:silas_malafaia</td>\n",
       "      <td>c:Olá povo abençoado do Brasil rapaz onde nós ...</td>\n",
       "      <td>Olá povo abençoado do Brasil rapaz onde nós ch...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\hashtagFALAMAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>t001821</td>\n",
       "      <td>v:cl_st1_guilherme-dataset-output\\MOTIVACIONAL...</td>\n",
       "      <td>d:2024-04-16</td>\n",
       "      <td>url:no_url</td>\n",
       "      <td>u:silas_malafaia</td>\n",
       "      <td>c:o deserto ninguém gosta dele deserto simboli...</td>\n",
       "      <td>o deserto ninguém gosta dele deserto simboliza...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>t001822</td>\n",
       "      <td>v:cl_st1_guilherme-dataset-output\\MOTIVACIONAL...</td>\n",
       "      <td>d:2024-04-16</td>\n",
       "      <td>url:no_url</td>\n",
       "      <td>u:silas_malafaia</td>\n",
       "      <td>c:e guarde a sua mente o que domina sua mente ...</td>\n",
       "      <td>e guarde a sua mente o que domina sua mente a ...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>t001823</td>\n",
       "      <td>v:cl_st1_guilherme-dataset-output\\MOTIVACIONAL...</td>\n",
       "      <td>d:2024-04-16</td>\n",
       "      <td>url:no_url</td>\n",
       "      <td>u:silas_malafaia</td>\n",
       "      <td>c:em Gênesis Capítulo 6 Versículo 8 Oi Noé por...</td>\n",
       "      <td>em Gênesis Capítulo 6 Versículo 8 Oi Noé porém...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>t001824</td>\n",
       "      <td>v:cl_st1_guilherme-dataset-output\\MOTIVACIONAL...</td>\n",
       "      <td>d:2024-04-16</td>\n",
       "      <td>url:no_url</td>\n",
       "      <td>u:silas_malafaia</td>\n",
       "      <td>c:a uma pessoa determinada é uma pessoa que sa...</td>\n",
       "      <td>a uma pessoa determinada é uma pessoa que sabe...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>t001825</td>\n",
       "      <td>v:cl_st1_guilherme-dataset-output\\MOTIVACIONAL...</td>\n",
       "      <td>d:2024-04-16</td>\n",
       "      <td>url:no_url</td>\n",
       "      <td>u:silas_malafaia</td>\n",
       "      <td>c:é a fé que podemos ter e a fé do apóstolo Pa...</td>\n",
       "      <td>é a fé que podemos ter e a fé do apóstolo Paul...</td>\n",
       "      <td>cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1826 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      text_id                                       conversation  \\\n",
       "0     t000000  v:cl_st1_guilherme-dataset-output\\hashtagFALAM...   \n",
       "1     t000001  v:cl_st1_guilherme-dataset-output\\hashtagFALAM...   \n",
       "2     t000002  v:cl_st1_guilherme-dataset-output\\hashtagFALAM...   \n",
       "3     t000003  v:cl_st1_guilherme-dataset-output\\hashtagFALAM...   \n",
       "4     t000004  v:cl_st1_guilherme-dataset-output\\hashtagFALAM...   \n",
       "...       ...                                                ...   \n",
       "1821  t001821  v:cl_st1_guilherme-dataset-output\\MOTIVACIONAL...   \n",
       "1822  t001822  v:cl_st1_guilherme-dataset-output\\MOTIVACIONAL...   \n",
       "1823  t001823  v:cl_st1_guilherme-dataset-output\\MOTIVACIONAL...   \n",
       "1824  t001824  v:cl_st1_guilherme-dataset-output\\MOTIVACIONAL...   \n",
       "1825  t001825  v:cl_st1_guilherme-dataset-output\\MOTIVACIONAL...   \n",
       "\n",
       "              date    text_url              user  \\\n",
       "0     d:2024-04-16  url:no_url  u:silas_malafaia   \n",
       "1     d:2024-04-16  url:no_url  u:silas_malafaia   \n",
       "2     d:2024-04-16  url:no_url  u:silas_malafaia   \n",
       "3     d:2024-04-16  url:no_url  u:silas_malafaia   \n",
       "4     d:2024-04-16  url:no_url  u:silas_malafaia   \n",
       "...            ...         ...               ...   \n",
       "1821  d:2024-04-16  url:no_url  u:silas_malafaia   \n",
       "1822  d:2024-04-16  url:no_url  u:silas_malafaia   \n",
       "1823  d:2024-04-16  url:no_url  u:silas_malafaia   \n",
       "1824  d:2024-04-16  url:no_url  u:silas_malafaia   \n",
       "1825  d:2024-04-16  url:no_url  u:silas_malafaia   \n",
       "\n",
       "                                                content  \\\n",
       "0     c:povo abençoado do Brasil há mais de 2 anos i...   \n",
       "1     c:estou emem Alagoas e primeiro eu quero agrad...   \n",
       "2     c:povo abençoado do Brasil Preste bastante ate...   \n",
       "3     c:povo abençoado do Brasil quando a gente pens...   \n",
       "4     c:Olá povo abençoado do Brasil rapaz onde nós ...   \n",
       "...                                                 ...   \n",
       "1821  c:o deserto ninguém gosta dele deserto simboli...   \n",
       "1822  c:e guarde a sua mente o que domina sua mente ...   \n",
       "1823  c:em Gênesis Capítulo 6 Versículo 8 Oi Noé por...   \n",
       "1824  c:a uma pessoa determinada é uma pessoa que sa...   \n",
       "1825  c:é a fé que podemos ter e a fé do apóstolo Pa...   \n",
       "\n",
       "                                                   text  \\\n",
       "0     povo abençoado do Brasil há mais de 2 anos ins...   \n",
       "1     estou emem Alagoas e primeiro eu quero agradec...   \n",
       "2     povo abençoado do Brasil Preste bastante atenç...   \n",
       "3     povo abençoado do Brasil quando a gente pensa ...   \n",
       "4     Olá povo abençoado do Brasil rapaz onde nós ch...   \n",
       "...                                                 ...   \n",
       "1821  o deserto ninguém gosta dele deserto simboliza...   \n",
       "1822  e guarde a sua mente o que domina sua mente a ...   \n",
       "1823  em Gênesis Capítulo 6 Versículo 8 Oi Noé porém...   \n",
       "1824  a uma pessoa determinada é uma pessoa que sabe...   \n",
       "1825  é a fé que podemos ter e a fé do apóstolo Paul...   \n",
       "\n",
       "                                               filepath  \n",
       "0     cl_st1_guilherme-dataset-output\\hashtagFALAMAL...  \n",
       "1     cl_st1_guilherme-dataset-output\\hashtagFALAMAL...  \n",
       "2     cl_st1_guilherme-dataset-output\\hashtagFALAMAL...  \n",
       "3     cl_st1_guilherme-dataset-output\\hashtagFALAMAL...  \n",
       "4     cl_st1_guilherme-dataset-output\\hashtagFALAMAL...  \n",
       "...                                                 ...  \n",
       "1821  cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...  \n",
       "1822  cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...  \n",
       "1823  cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...  \n",
       "1824  cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...  \n",
       "1825  cl_st1_guilherme-dataset-output\\MOTIVACIONAL P...  \n",
       "\n",
       "[1826 rows x 8 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f23e3-bc12-4150-8bf1-557184b40d1b",
   "metadata": {},
   "source": [
    "### Creating the file `file_index.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "199bbbe5-a366-4fde-9314-f0de3d49c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered[['text_id', 'conversation', 'date', 'text_url']].to_csv('file_index.txt', sep=' ', index=False, header=False, encoding='utf-8', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd415c21-39dc-4083-9632-e5cd92fa6948",
   "metadata": {},
   "source": [
    "### Creating the file `tweets.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5445fc96-0b9f-4283-80b3-09b1bddeab91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder tweets created!\n"
     ]
    }
   ],
   "source": [
    "folder = 'tweets'\n",
    "try:\n",
    "    os.mkdir(folder)\n",
    "    print(f'Folder {folder} created!')\n",
    "except FileExistsError:\n",
    "    print(f'Folder {folder} already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de49e26-5efe-472f-9952-cef41fcda082",
   "metadata": {},
   "source": [
    "Note: The parameters `doublequote=False` and `escapechar=' '` are required to avoid that the column content is doublequoted with '\"' in sentences that use characters that need to be escaped such as double quote '\"' itself - this causes a malformed response from TreeTagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b566e03-56e4-44a8-8558-b6e940bb6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered[['text_id', 'conversation', 'date', 'user', 'content']].to_csv(f'{folder}/tweets.txt', sep='|', index=False, header=False, encoding='utf-8', lineterminator='\\n', doublequote=False, escapechar=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f769843-4ff9-4436-a2e0-d61e2c391e9d",
   "metadata": {},
   "source": [
    "## Tagging with TreeTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3877be9-6317-42ee-8e8d-c15c48f77996",
   "metadata": {},
   "source": [
    "- On Visual Studio Code (VS Code), open the folder where your project is located with `Open Folder...`\n",
    "- Open a WSL Ubuntu Terminal on VS Code\n",
    "- **Important**: Activate the `my_env` Python environment by executing `source \"$HOME\"/my_env/bin/activate`\n",
    "- Proceed as indicated\n",
    "\n",
    "Note: You have to download and open this Jupyter Notebook on JupyterLab (provided as part of Anaconda Distribution) to visualise the procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720724ab-5e77-4e9e-b05a-24cf73259cd0",
   "metadata": {},
   "source": [
    "Purpose: Annotate the texts in `tweets/tweets.txt` with part-of-speech and lemma information.\n",
    "- Input\n",
    "    - `file_index.txt`\n",
    "    - `tweets/tweets.txt`\n",
    "- Output\n",
    "    - `tweets/tagged.txt`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9005cc09-f5ef-454f-9b74-4d7ca9fe4f10",
   "metadata": {},
   "source": [
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ source \"$HOME\"/my_env/bin/activate\n",
    "(my_env) eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ sh treetagging.sh\n",
    "--- treetagging t000000 / t018205 ---\n",
    "        reading parameters ...\n",
    "        tagging ...\n",
    "         finished.\n",
    "--- treetagging t000001 / t018205 ---\n",
    "        reading parameters ...\n",
    "        tagging ...\n",
    "         finished.\n",
    "--- treetagging t000002 / t018205 ---\n",
    "        reading parameters ...\n",
    "        tagging ...\n",
    "         finished.\n",
    "--- treetagging t000003 / t018205 ---\n",
    "        reading parameters ...\n",
    "        tagging ...\n",
    "         finished.\n",
    "<omitted>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0060ba-d01c-4e48-a353-8c9ba0fc456e",
   "metadata": {},
   "source": [
    "## Processing `tokenstypes`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93351bf2-3457-4b47-8a47-7ff13c4e0e1a",
   "metadata": {},
   "source": [
    "Purpose: Capture the content tokens (specific occurrences of words) and the content types (general concept of words) from `tweets/tagged.txt`.\n",
    "- Input\n",
    "    - `file_index.txt`\n",
    "    - `tweets/tagged.txt`\n",
    "- Output\n",
    "    - `tweets/tokens.txt`\n",
    "    - `tweets/types.txt`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cddc717-8ff2-4f38-ae10-8d6616de6a6c",
   "metadata": {},
   "source": [
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ source \"$HOME\"/my_env/bin/activate\n",
    "(my_env) eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ sh tokenstypes.sh\n",
    "--- tokenstypes t000000 / 18206 ---\n",
    "--- tokenstypes t000001 / 18206 ---\n",
    "--- tokenstypes t000002 / 18206 ---\n",
    "--- tokenstypes t000003 / 18206 ---\n",
    "--- tokenstypes t000004 / 18206 ---\n",
    "--- tokenstypes t000005 / 18206 ---\n",
    "<omitted>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef598329-fb5a-4b07-8799-b2bbb55847f1",
   "metadata": {},
   "source": [
    "## Processing `toplemmas`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce5796-0434-44d5-88e2-8f934a6cebc8",
   "metadata": {},
   "source": [
    "Purpose: Determine the 1.000 top lemmas. **Important**: This process requires manual inspection. Non-meaningful lemmas should be excluded by updating `stoplist.sed` and reiterating the processing.\n",
    "- Input\n",
    "    - `tweets/types.txt`\n",
    "    - `stoplist.sed`: List of rules that allows the exclusion of a certain lemmas\n",
    "- Output\n",
    "    - `selectedwords` = `var_index.txt`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13b0fc8e-73e2-4e2d-9e2f-73068e4132fc",
   "metadata": {},
   "source": [
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ source \"$HOME\"/my_env/bin/activate\n",
    "(my_env) eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ sh toplemmas.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a4675-5067-4e8b-8ca8-baf9f1b915c1",
   "metadata": {},
   "source": [
    "## Processing `sas`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d158b2-73d7-477e-a7a7-2b87216a19ae",
   "metadata": {},
   "source": [
    "Purpose: Prepare input data for processing in SAS.\n",
    "- Input\n",
    "    - `tweets/types.txt`\n",
    "    - `selectedwords`\n",
    "    - `file_index.txt`\n",
    "- Output\n",
    "    - `columns`\n",
    "    - `sas/data.txt`\n",
    "    - `sas/dates.txt`\n",
    "    - `sas/wcount.txt`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d250d29-861c-41a6-9793-e0df758100e0",
   "metadata": {},
   "source": [
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ source \"$HOME\"/my_env/bin/activate\n",
    "(my_env) eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ sh sas.sh\n",
    "--- v000001 ---\n",
    "--- v000002 ---\n",
    "--- v000003 ---\n",
    "--- v000004 ---\n",
    "--- v000005 ---\n",
    "<omitted>\n",
    "--- v001000 ---\n",
    "[nltk_data] Downloading package punkt to /home/eyamrog/nltk_data...\n",
    "[nltk_data]   Package punkt is already up-to-date!\n",
    "Word counts written to sas/wcount.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8087ea-ffd4-43b9-8eda-4444b931c319",
   "metadata": {},
   "source": [
    "## Processing `datamatrix`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4374f48-04b6-48af-a31d-125ef0e85972",
   "metadata": {},
   "source": [
    "Purpose: Prepares input data for calculating the correlation matrix.\n",
    "- Input\n",
    "    - `file_index.txt`\n",
    "    - `columns`\n",
    "    - `selectedwords`\n",
    "- Output\n",
    "    - `file_ids.txt`\n",
    "    - `data.csv`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23925f09-05ef-44a5-ba48-78cac1dedc76",
   "metadata": {},
   "source": [
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ source \"$HOME\"/my_env/bin/activate\n",
    "(my_env) eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ sh datamatrix.sh\n",
    "(my_env) eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ sh datamatrix.sh\n",
    "--- v000001 ---\n",
    "--- v000002 ---\n",
    "--- v000003 ---\n",
    "--- v000004 ---\n",
    "--- v000005 ---\n",
    "<omitted>\n",
    "--- v001000 ---\n",
    "--- data.csv ...---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6130e8f0-a7e5-4f6e-b427-090f7cda4856",
   "metadata": {},
   "source": [
    "## Processing `correlationmatrix`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bddbb87-b4de-4c41-8177-b2f677d803b6",
   "metadata": {},
   "source": [
    "Purpose: Calculates the correlation matrix.\n",
    "- Input\n",
    "    - `data.csv`\n",
    "- Output\n",
    "    - `correlation`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42927846-5d87-4f5f-8ce6-8beb4ef733a4",
   "metadata": {},
   "source": [
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ source \"$HOME\"/my_env/bin/activate\n",
    "(my_env) eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ sh correlationmatrix.sh\n",
    "--- python correlation ... ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9203f-e9e1-4a73-8bed-7dec1ad8617e",
   "metadata": {},
   "source": [
    "## Processing `formats`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69517100-c28d-44ba-90a7-37988e3df8ad",
   "metadata": {},
   "source": [
    "Purpose: Prepare input data for processing in SAS.\n",
    "- Input\n",
    "    - `data.csv`\n",
    "    - `selectedwords`\n",
    "- Output\n",
    "    - `sas/corr.txt`\n",
    "    - `sas/word_labels_format.sas`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96f9b02f-fd33-4ff2-bf57-463fac9743e6",
   "metadata": {},
   "source": [
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ source \"$HOME\"/my_env/bin/activate\n",
    "(my_env) eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ sh formats.sh\n",
    "--- sas/sas/corr.txt ---\n",
    "--- sas/word_labels_format.sas ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd38617b-0487-4ba2-b619-d9b78aba9f2c",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf501e4a-1193-4999-bdc0-88c08d2840f7",
   "metadata": {},
   "source": [
    "Right-click on the link and choose `Save link as` to download the corresponding file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73ba45-9d6c-4578-b14e-c80c3ccaafe4",
   "metadata": {},
   "source": [
    "- [CL_St1_Querem_Results.zip](https://laelgelcquerem.s3.sa-east-1.amazonaws.com/CL_St1_Querem_Results.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb169464-4f29-4513-b272-46f201deff17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
